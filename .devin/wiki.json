{
  "repo_notes": [
    {
      "content": "This is a GitOps-managed Kubernetes home lab running on Talos Linux with Flux CD. The cluster is named 'titans' and consists of 5 nodes (3 control-plane: k8s-oceanus, k8s-tethys, k8s-coeus; 2 workers: k8s-chronos, k8s-rhea). The primary entry point is kubernetes/flux/apps.yaml which aggregates all applications from kubernetes/apps/. Documentation should emphasize the GitOps workflow, variable substitution patterns, and the layered Kustomization structure."
    },
    {
      "content": "Key architectural patterns: 1) Apps live in kubernetes/apps/<category>/<app>/app/ with helmrelease.yaml and kustomization.yaml. 2) Flux variable substitution uses cluster-settings (ConfigMap), cluster-secrets (SOPS-encrypted Secret), and cluster-ipam (ConfigMap) for values like ${HOME_DOMAIN}, ${IPAM_IP_*}. 3) Secrets use SOPS+Age encryption with ExternalSecrets operator pulling from 1Password Connect. 4) Storage uses Rook-Ceph (ceph-block default, ceph-filesystem, ceph-bucket) and OpenEBS (hostpath for local). 5) VolSync handles PVC backup/restore to NFS and R2."
    },
    {
      "content": "The talos/ directory contains Talos Linux configuration using talhelper. The talconfig.yaml defines the cluster topology, and patches/ contains machine configuration patches. Bootstrap uses helmfile.yaml to install critical components (Cilium, CoreDNS, Spegel) before Flux takes over. Networking uses Cilium CNI with L2 announcements for LoadBalancer IPs in the 10.100.47.200-250 range."
    },
    {
      "content": "Ingress is split into internal (default, via ingress-nginx-internal at ${IPAM_IP_NGINX_INTERNAL}) and external (via Cloudflare Tunnels for zero-trust access). TLS certificates are managed by cert-manager with Let's Encrypt. External-DNS automatically creates DNS records in Cloudflare and UniFi. The kubernetes/templates/ folder contains reusable Kustomize components for Gatus health checks and VolSync backup configurations."
    }
  ],
  "pages": [
    {
      "title": "Overview",
      "purpose": "Introduce the home-k8s repository as a GitOps-managed Kubernetes home lab running on Talos Linux with Flux CD, including high-level architecture diagram showing the cluster topology and key technologies",
      "page_notes": [
        {
          "content": "Cover: Cluster name 'titans' with 5 nodes (3 control-plane, 2 workers) on 10.100.47.x network. Key technologies: Talos Linux v1.11+, Kubernetes v1.34+, Flux CD, Cilium CNI, Rook-Ceph storage, 1Password for secrets. Entry point is kubernetes/flux/apps.yaml. Mention the GitHub repo structure: kubernetes/ (apps, flux, templates), talos/ (OS config), .github/workflows (CI)."
        }
      ]
    },
    {
      "title": "Getting Started",
      "purpose": "Guide for setting up the development environment and understanding the bootstrap process",
      "page_notes": [
        {
          "content": "Prerequisites: age.key for SOPS decryption, kubeconfig, talosconfig. The .mise.toml defines required tools: kubectl, sops, age, task, flux2, talhelper, talosctl, krew. Environment variables set KUBECONFIG, SOPS_AGE_KEY_FILE, TALOSCONFIG paths. Taskfile.yaml provides common automation tasks."
        }
      ]
    },
    {
      "title": "Development Environment & Bootstrap",
      "purpose": "Detailed explanation of the mise toolchain, required tools, and step-by-step bootstrap process for the cluster",
      "parent": "Getting Started",
      "page_notes": [
        {
          "content": "Tools in .mise.toml: kubectl 1.35.0, sops 3.11.0, age 1.2.1, task 3.44.0, flux2 2.7.5, talhelper latest, talosctl 1.11.6. Environment paths: kubernetes/kubeconfig, age.key at root, talos/clusterconfig/talosconfig. Bootstrap order: 1) Generate Talos configs with talhelper. 2) Apply configs to nodes. 3) Run helmfile (talos/helmfile.yaml) for cilium, coredns, kubelet-csr-approver, spegel. 4) Bootstrap Flux with sops-age secret. 5) Flux applies kubernetes/flux/config/ then apps.yaml."
        }
      ]
    },
    {
      "title": "Secret Management",
      "purpose": "Explain the SOPS + Age encryption system, ExternalSecrets operator with 1Password Connect, and how secrets flow in GitOps",
      "parent": "Getting Started",
      "page_notes": [
        {
          "content": "Two secret mechanisms: 1) SOPS with Age encryption for in-repo secrets (*.sops.yaml files). The .sops.yaml defines regex patterns - kubernetes/ encrypts data/stringData fields, talos/ encrypts entire files. Age public key: age1tk7tw0625mspavxe8lvv348p58ydqw6r04sxcjq76m98nan2lsnq3dhvxd. 2) ExternalSecrets operator with ClusterSecretStore pointing to 1Password Connect (http://onepassword-connect.op:8080). Apps use ExternalSecret resources referencing 1Password vault items."
        }
      ]
    },
    {
      "title": "Architecture",
      "purpose": "Comprehensive overview of the system architecture, deployment patterns, and design decisions with diagrams",
      "page_notes": [
        {
          "content": "Include architecture diagram showing: Control plane VIP at 10.100.47.10, node IPs (oceanus .35, tethys .48, coeus .49, chronos .50, rhea .51), pod CIDR 10.69.0.0/16, service CIDR 10.96.0.0/16. LoadBalancer IP pool 10.100.47.200-250. NFS at 10.100.47.100. Show Flux reconciliation flow from Git to cluster. Highlight separation of concerns: kube-system (CNI, DNS), rook-ceph (storage), observability (monitoring), network (ingress), apps (workloads)."
        }
      ]
    },
    {
      "title": "GitOps & CI/CD",
      "purpose": "Detailed documentation of the GitOps workflow using Flux CD, GitHub Actions validation, and automated dependency updates",
      "parent": "Architecture",
      "page_notes": [
        {
          "content": "Flux structure: kubernetes/flux/config/ (cluster.yaml, flux.yaml), kubernetes/flux/repositories/ (helm, git, oci sources), kubernetes/flux/vars/ (cluster-settings, cluster-secrets, cluster-ipam). The apps.yaml Kustomization patches all child Kustomizations with decryption and postBuild.substituteFrom unless labeled substitution.flux.home.arpa/disabled: true. GitHub Actions: meta-flux-diff.yaml runs flux-local test and diff on PRs, flux-failure-issue.yaml creates issues on Flux errors. Renovate handles dependency updates."
        }
      ]
    },
    {
      "title": "Infrastructure Components",
      "purpose": "Documentation of core infrastructure: Talos Linux, Cilium CNI, CoreDNS, cert-manager, Spegel registry mirroring, and system upgrades via TUPPR",
      "parent": "Architecture",
      "page_notes": [
        {
          "content": "Talos: talconfig.yaml defines nodes with static IPs, VIP on control-plane. Patches in talos/patches/global/ and patches/controlplane/. Cilium: Host networking, L2 announcements, LoadBalancer IPs. CoreDNS with custom Corefile. Spegel for P2P container images. Kubelet-csr-approver for cert approval. System upgrades via TUPPR (system-upgrade/tuppr/) with TalosUpgrade and KubernetesUpgrade CRs coordinated with Renovate."
        }
      ]
    },
    {
      "title": "Storage",
      "purpose": "Overview of the storage architecture including Rook-Ceph distributed storage, OpenEBS local volumes, and VolSync backups",
      "page_notes": [
        {
          "content": "Primary storage: Rook-Ceph on /dev/sda devices across nodes. OpenEBS hostpath for local volumes. VolSync for backup/restore with restic to NFS (10.100.47.100) and Cloudflare R2. Snapshot-controller enables CSI snapshots. Storage dependencies: most apps dependsOn rook-ceph-cluster and volsync."
        }
      ]
    },
    {
      "title": "Rook-Ceph Cluster",
      "purpose": "Comprehensive guide to Rook-Ceph: operator deployment, cluster configuration, storage classes, pools, snapshots, and monitoring",
      "parent": "Storage",
      "page_notes": [
        {
          "content": "Rook-Ceph in kubernetes/apps/rook-ceph/: operator and cluster. Cluster uses host networking, deviceFilter: sda, 3x replicated pools. MGR/MON on control-plane. Dashboard at rook.${HOME_DOMAIN}. StorageClasses: ceph-block (default, RWO), ceph-filesystem (RWX), ceph-bucket (S3 via ObjectBucketClaim). VolumeSnapshotClasses: csi-ceph-blockpool, csi-ceph-filesystem. Object store gateway at rgw.${HOME_DOMAIN}. Prometheus monitoring enabled."
        }
      ]
    },
    {
      "title": "Backup & Disaster Recovery",
      "purpose": "Guide to VolSync backup strategies, Ceph snapshots, PostgreSQL PITR with barman-cloud, and recovery procedures",
      "parent": "Storage",
      "page_notes": [
        {
          "content": "Backup strategies: 1) VolSync ReplicationSource to NFS and R2 for PVCs - templates in kubernetes/templates/volsync/. 2) Ceph VolumeSnapshots. 3) CNPG continuous backup to R2 via barman-cloud with PITR. Recovery: VolSync ReplicationDestination restores PVCs, CNPG bootstrap.recovery section for database. Apps include volsync.yaml referencing templates."
        }
      ]
    },
    {
      "title": "Networking",
      "purpose": "Overview of network architecture including Cilium CNI, load balancing, DNS, and external access patterns",
      "page_notes": [
        {
          "content": "Network topology: Node network 10.100.47.0/24, gateway 10.100.47.1. Pod CIDR 10.69.0.0/16, Service CIDR 10.96.0.0/16. LoadBalancer pool 10.100.47.200-250 via Cilium L2. Key IPs: Plex .230, NGINX external .249, internal .250, AdGuard .200-.202, Postgres .220. Ingress split: internal (default) vs external (Cloudflare Tunnel)."
        }
      ]
    },
    {
      "title": "CNI & Load Balancing",
      "purpose": "Detailed documentation of Cilium CNI, L2 announcements, IPAM, LoadBalancer implementation, and network policies",
      "parent": "Networking",
      "page_notes": [
        {
          "content": "Cilium in kube-system/cilium/: app/ (helmrelease, helm-values), config/ (l2announcementpolicy, loadbalancerippool, localredirectpolicy, peeringpolicy). CNI mode: none in Talos (Cilium takes over). CiliumLoadBalancerIPPool defines 10.100.47.200-250. Services request IPs via io.cilium/lb-ipam-ips annotation. Hubble UI for observability. Kyverno policies for pod security in kubernetes/apps/kyverno/policies/."
        }
      ]
    },
    {
      "title": "DNS & Ingress",
      "purpose": "Documentation of AdGuard Home HA, CoreDNS, External-DNS (Cloudflare/UniFi), ingress-nginx internal/external, Cloudflare Tunnels, and cert-manager TLS",
      "parent": "Networking",
      "page_notes": [
        {
          "content": "DNS layers: 1) AdGuard Home 3 replicas at .200-.202 for ad blocking. 2) CoreDNS cluster DNS. 3) External-DNS with Cloudflare + UniFi providers. Ingress: ingress-nginx internal (default, .250) and external (.249). Cloudflared tunnels for zero-trust external access. Cert-manager with Let's Encrypt ClusterIssuers. Apps use external-dns.alpha.kubernetes.io/target annotation and className: internal or tunnel via cloudflared."
        }
      ]
    },
    {
      "title": "Data Services",
      "purpose": "Overview of database and caching services: PostgreSQL (CloudNative-PG) and Dragonfly (Redis-compatible)",
      "page_notes": [
        {
          "content": "Database namespace: cloudnative-pg/ (PostgreSQL operator + cluster), dragonfly/ (Redis-compatible). Both provide HA with automatic failover. Apps use init containers (ghcr.io/home-operations/postgres-init). Pooler (PgBouncer) available. LoadBalancer at ${IPAM_IP_POSTGRES}=10.100.47.220."
        }
      ]
    },
    {
      "title": "PostgreSQL (CloudNative-PG)",
      "purpose": "Documentation of CloudNative-PG operator, PostgreSQL 17 cluster with HA, backup to R2, pooler, and recovery procedures",
      "parent": "Data Services",
      "page_notes": [
        {
          "content": "CNPG in database/cloudnative-pg/: app/ (operator), cluster/ (cluster17.yaml, pooler.yaml, scheduledbackup.yaml, objectstore-r2.yaml). Cluster: 3 instances, PostgreSQL 17.4, 20Gi storage + 5Gi WAL on ceph-block. Backup via barman-cloud to R2 (serverName: postgres17-v1). Features: hugepages-2Mi, max_connections 250, PodMonitor. Recovery: uncomment bootstrap.recovery, set externalClusters. See README.md."
        }
      ]
    },
    {
      "title": "Dragonfly (Redis-compatible)",
      "purpose": "Guide to Dragonfly operator and cluster: deployment, 3-replica HA, emulated cluster mode, application usage",
      "parent": "Data Services",
      "page_notes": [
        {
          "content": "Dragonfly in database/dragonfly/: app/ (operator via app-template with RBAC), cluster/ (Dragonfly CR, PodMonitor). Cluster: 3 replicas, 512Mi memory, emulated cluster mode with lock_on_hashtags. Apps connect via dragonfly.database.svc.cluster.local:6379 for Redis-compatible caching."
        }
      ]
    },
    {
      "title": "Observability",
      "purpose": "Overview of the observability stack: Prometheus, Alertmanager, Loki, Vector, Grafana, Gatus, and exporters",
      "page_notes": [
        {
          "content": "Observability namespace: prometheus-operator-crds, kube-prometheus-stack, loki, vector, grafana, gatus, smartctl-exporter, unpoller, tautulli, kuberhealthy. Logs: containers → Vector agent → aggregator → Loki. Metrics: Prometheus scrapes ServiceMonitors. Alerts via Alertmanager to Pushover. Gatus for health checks with templates in kubernetes/templates/gatus/."
        }
      ]
    },
    {
      "title": "Metrics & Alerting",
      "purpose": "Detailed documentation of kube-prometheus-stack, Prometheus, Alertmanager with Pushover, PrometheusRules, and heartbeat monitoring",
      "parent": "Observability",
      "page_notes": [
        {
          "content": "kube-prometheus-stack deploys Prometheus, Alertmanager, node-exporter, kube-state-metrics. ServiceMonitors auto-discover. PrometheusRules for alerts. Alertmanager config: routes by severity, receivers for null, heartbeat (Watchdog), pushover (critical). Pushover uses ALERTMANAGER_PUSHOVER_TOKEN. Heartbeat every 5m. Additional exporters: smartctl-exporter, unpoller."
        }
      ]
    },
    {
      "title": "Logging & Visualization",
      "purpose": "Guide to Loki with S3 backend, Vector agent/aggregator pipeline, Grafana dashboards, and Gatus status monitoring",
      "parent": "Observability",
      "page_notes": [
        {
          "content": "Loki in observability/loki/ with ObjectBucketClaim for S3 storage. Vector: agent DaemonSet → aggregator StatefulSet (LB at .214) → Loki. Grafana at grafana.${HOME_DOMAIN} with Prometheus, Loki, Alertmanager datasources. Gatus health checks: templates in kubernetes/templates/gatus/ (external, guarded, infrastructure). ConfigMaps with gatus.io/enabled: true auto-discovered."
        }
      ]
    },
    {
      "title": "Applications",
      "purpose": "Overview of user-facing applications organized by category with common deployment patterns",
      "page_notes": [
        {
          "content": "Categories: arrs/ (media automation), media/ (Plex, Stash), home-assistant/, games/ (Minecraft, RomM), services/ (Atuin, static-webserver), renovate/. Common patterns: app-template helm chart, dependsOn rook-ceph-cluster + volsync, postgres-init container, reloader.stakater.com/auto annotation, NFS at /mnt/user/Media."
        }
      ]
    },
    {
      "title": "Media Stack",
      "purpose": "Comprehensive guide to the Arrs ecosystem (Prowlarr, Sonarr, Radarr, Bazarr, SABnzbd, Seerr) and media servers (Plex, Stash, Tautulli)",
      "parent": "Applications",
      "page_notes": [
        {
          "content": "Arrs in kubernetes/apps/arrs/: prowlarr, sonarr, radarr, radarr-3d, whisparr, bazarr (with subcleaner), sabnzbd, seerr, wizarr, profilarr. All use app-template, PostgreSQL, NFS media at ${IPAM_IP_NFS}:/mnt/user/Media. Pushover notifications. Media servers: plex (LB at .230, LokiRule), stash (VolSync backup), tautulli (Kuberhealthy check). Internal ingress at <app>.${HOME_DOMAIN}."
        }
      ]
    },
    {
      "title": "Home Automation",
      "purpose": "Guide to Home Assistant deployment with Code Server sidecar, PostgreSQL backend, HACS integration",
      "parent": "Applications",
      "page_notes": [
        {
          "content": "Home Assistant in home-assistant/: init containers for postgres-init and HACS. Main: ghcr.io/home-operations/home-assistant. Sidecar: code-server at code.ha.${HOME_DOMAIN}. Persistence: config PVC with VolSync, emptyDir for logs/tts/tmp. Ingress at ha.${HOME_DOMAIN}. Descheduler eviction disabled."
        }
      ]
    },
    {
      "title": "Network Services",
      "purpose": "Documentation of AdGuard Home HA DNS, Cloudflared tunnels, DDNS, SMTP relay, and UniFi monitoring",
      "parent": "Applications",
      "page_notes": [
        {
          "content": "AdGuard Home (3 replicas at .200-.202 for HA DNS filtering). Cloudflared (2 replicas, QUIC, post-quantum). Cloudflare-ddns for dynamic DNS. SMTP-relay with Maddy. Unpoller in observability/ for UniFi metrics. Echo-server for testing."
        }
      ]
    },
    {
      "title": "Developer Tools",
      "purpose": "Guide to self-hosted Renovate, Atuin shell history, GitHub Actions Runner Controller, and game servers",
      "parent": "Applications",
      "page_notes": [
        {
          "content": "Renovate in renovate/ with S3 cache via ObjectBucketClaim, config in app/config/. Atuin in services/atuin/ for shell history sync. GitHub Actions Runner Controller in actions-runner-system/ with ghar-scale-set (runners named 'ghar-set-zoo'). Games: mc-router (Minecraft proxy at .211), minecraft-stoneblock4, romm (ROM manager with PostgreSQL)."
        }
      ]
    },
    {
      "title": "Security & RBAC",
      "purpose": "Documentation of Kyverno policies, pod security contexts, RBAC patterns, and Reflector for secret mirroring",
      "page_notes": [
        {
          "content": "Kyverno in kubernetes/apps/kyverno/ with policies: defaultPodResources, disallowEmptyIngress, externalsecret-usage-tracking, safeToEvict, volsync-movers. All apps use: runAsNonRoot, readOnlyRootFilesystem, drop ALL capabilities, seccompProfile RuntimeDefault, runAsUser/runAsGroup 568. Reflector (tools/reflector/) mirrors secrets across namespaces."
        }
      ]
    }
  ]
}
